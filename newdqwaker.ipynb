{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frozen-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dropout,Dense,Input,Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class Walker:\n",
    "    def __init__(self,nx,ny,lr,gamma):\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        self.lr = lr\n",
    "        self.los = []\n",
    "        self.gamma = gamma\n",
    "        self.memory_deck = deque(maxlen=2000)\n",
    "        self.epsilon = 0.7\n",
    "        self.epsilon_ = 0.01\n",
    "        self.decay = 0.995\n",
    "        self.model = self.get_model()\n",
    "        self.episode_observation, self.episode_rewards, self.episode_action, self.new_episode_observation,self.episode_flag = [],[],[],[],[]\n",
    "\n",
    "    def get_action(self,observation):\n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return np.random.uniform(-1,1,4)\n",
    "        p = self.model.predict(observation)\n",
    "        return p[0]\n",
    "        \n",
    "    def memory_recall(self,observation,action,reward,new_observation,flags):\n",
    "        self.memory_deck.append((observation,action,reward,new_observation,flags))\n",
    "        self.episode_rewards.append(reward)\n",
    "    \n",
    "    def get_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(400,input_dim=self.nx,activation='relu'))\n",
    "        model.add(Dense(300,activation='relu'))\n",
    "        model.add(Dense(self.ny,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "    \n",
    "    def training(self,batch):\n",
    "        i = random.sample(self.memory_deck,batch)\n",
    "        self.los = []\n",
    "        for obs,act,rew,new_obs,done in i:\n",
    "            target = rew\n",
    "            if not done:\n",
    "                target = ((1.0-0.1)*rew+0.1*(self.gamma*np.amax(self.model.predict(new_obs)[0])))\n",
    "            \n",
    "            old_target = self.model.predict(obs)\n",
    "            old_target[0] = target\n",
    "            history = self.model.fit(x=obs,y=old_target,verbose=0,epochs=1)\n",
    "            self.los.append(history.history['loss'])\n",
    "            self.episode_observation, self.episode_rewards, self.episode_action, self.new_episode_observation,self.episode_flag = [],[],[],[],[]\n",
    "\n",
    "        mm = np.mean(self.los)\n",
    "        if self.epsilon>=self.epsilon_:\n",
    "            self.epsilon*=self.decay\n",
    "        return history,mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jln766\\Anaconda3\\envs\\DL525\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Walk# :  0\n",
      "Reward :  -85\n",
      "Time :  20.03 sec\n",
      "Maximum Reward : -85       (in episode#:0)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  1\n",
      "Reward :  -90\n",
      "Time :  20.0 sec\n",
      "Maximum Reward : -85       (in episode#:0)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  2\n",
      "Reward :  -81\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -81       (in episode#:2)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  3\n",
      "Reward :  -113\n",
      "Time :  20.02 sec\n",
      "Maximum Reward : -81       (in episode#:2)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  4\n",
      "Reward :  -73\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  5\n",
      "Reward :  -78\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  6\n",
      "Reward :  -80\n",
      "Time :  20.02 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  7\n",
      "Reward :  -84\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  8\n",
      "Reward :  -123\n",
      "Time :  0.99 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  9\n",
      "Reward :  -100\n",
      "Time :  0.51 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  10\n",
      "Reward :  -105\n",
      "Time :  0.87 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  11\n",
      "Reward :  -119\n",
      "Time :  0.89 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  12\n",
      "Reward :  -121\n",
      "Time :  0.64 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  13\n",
      "Reward :  -100\n",
      "Time :  1.11 sec\n",
      "Maximum Reward : -73       (in episode#:4)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  14\n",
      "Reward :  -72\n",
      "Time :  20.02 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  15\n",
      "Reward :  -117\n",
      "Time :  0.91 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  16\n",
      "Reward :  -98\n",
      "Time :  1.91 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  17\n",
      "Reward :  -99\n",
      "Time :  1.33 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  18\n",
      "Reward :  -104\n",
      "Time :  0.84 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  19\n",
      "Reward :  -99\n",
      "Time :  0.83 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  20\n",
      "Reward :  -99\n",
      "Time :  0.78 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  21\n",
      "Reward :  -99\n",
      "Time :  1.03 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  22\n",
      "Reward :  -114\n",
      "Time :  0.74 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  23\n",
      "Reward :  -97\n",
      "Time :  0.79 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  24\n",
      "Reward :  -123\n",
      "Time :  1.39 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  25\n",
      "Reward :  -102\n",
      "Time :  0.96 sec\n",
      "Maximum Reward : -72       (in episode#:14)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  26\n",
      "Reward :  -55\n",
      "Time :  20.02 sec\n",
      "Maximum Reward : -55       (in episode#:26)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  27\n",
      "Reward :  -52\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  28\n",
      "Reward :  -99\n",
      "Time :  0.62 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  29\n",
      "Reward :  -110\n",
      "Time :  0.69 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  30\n",
      "Reward :  -100\n",
      "Time :  1.17 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  31\n",
      "Reward :  -98\n",
      "Time :  0.82 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  32\n",
      "Reward :  -106\n",
      "Time :  1.3 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  33\n",
      "Reward :  -100\n",
      "Time :  0.97 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  34\n",
      "Reward :  -120\n",
      "Time :  1.98 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  35\n",
      "Reward :  -96\n",
      "Time :  0.88 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  36\n",
      "Reward :  -118\n",
      "Time :  0.62 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  37\n",
      "Reward :  -69\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  38\n",
      "Reward :  -107\n",
      "Time :  0.62 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  39\n",
      "Reward :  -96\n",
      "Time :  1.21 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  40\n",
      "Reward :  -120\n",
      "Time :  0.98 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  41\n",
      "Reward :  -107\n",
      "Time :  1.27 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  42\n",
      "Reward :  -119\n",
      "Time :  0.66 sec\n",
      "Maximum Reward : -52       (in episode#:27)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  43\n",
      "Reward :  -45\n",
      "Time :  20.02 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  44\n",
      "Reward :  -99\n",
      "Time :  1.03 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  45\n",
      "Reward :  -97\n",
      "Time :  0.87 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  46\n",
      "Reward :  -48\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  47\n",
      "Reward :  -100\n",
      "Time :  0.87 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  48\n",
      "Reward :  -100\n",
      "Time :  1.15 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  49\n",
      "Reward :  -98\n",
      "Time :  0.91 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  50\n",
      "Reward :  -101\n",
      "Time :  0.64 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Walk# :  51\n",
      "Reward :  -100\n",
      "Time :  0.41 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  52\n",
      "Reward :  -104\n",
      "Time :  1.23 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  53\n",
      "Reward :  -98\n",
      "Time :  1.32 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  54\n",
      "Reward :  -47\n",
      "Time :  20.03 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  55\n",
      "Reward :  -111\n",
      "Time :  0.87 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  56\n",
      "Reward :  -64\n",
      "Time :  20.0 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  57\n",
      "Reward :  -98\n",
      "Time :  1.35 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  58\n",
      "Reward :  -102\n",
      "Time :  0.81 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  59\n",
      "Reward :  -98\n",
      "Time :  0.91 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  60\n",
      "Reward :  -99\n",
      "Time :  0.8 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  61\n",
      "Reward :  -96\n",
      "Time :  1.01 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  62\n",
      "Reward :  -98\n",
      "Time :  1.3 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  63\n",
      "Reward :  -104\n",
      "Time :  1.49 sec\n",
      "Maximum Reward : -45       (in episode#:43)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  64\n",
      "Reward :  -28\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -28       (in episode#:64)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  65\n",
      "Reward :  -99\n",
      "Time :  1.0 sec\n",
      "Maximum Reward : -28       (in episode#:64)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  66\n",
      "Reward :  -97\n",
      "Time :  1.16 sec\n",
      "Maximum Reward : -28       (in episode#:64)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  67\n",
      "Reward :  -96\n",
      "Time :  0.99 sec\n",
      "Maximum Reward : -28       (in episode#:64)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  68\n",
      "Reward :  -30\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -28       (in episode#:64)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  69\n",
      "Reward :  -27\n",
      "Time :  20.01 sec\n",
      "Maximum Reward : -27       (in episode#:69)\n",
      "Wins : 0\n",
      "##################################################################\n",
      "Walk# :  70\n",
      "Reward :  -96\n",
      "Time :  1.34 sec\n",
      "Maximum Reward : -27       (in episode#:69)\n",
      "Wins : 0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "seed = np.random.seed(666)\n",
    "\n",
    "episodes = 10000\n",
    "render = False\n",
    "\n",
    "env = gym.make('BipedalWalker-v3')\n",
    "env = env.unwrapped\n",
    "\n",
    "lr = 0.001\n",
    "gamma = 0.98\n",
    "nx = env.observation_space.shape[0]\n",
    "ny = env.action_space.shape[0]\n",
    "agent = Walker(nx,ny,lr,gamma)\n",
    "win=0\n",
    "rewards_over_time = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    observation = env.reset()\n",
    "    observation = observation.reshape(1,-1)\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        if render==True:\n",
    "            env.render()\n",
    "        \n",
    "        action = agent.get_action(observation)\n",
    "        new_observation,reward,flag,inf = env.step(action)\n",
    "        new_observation = new_observation.reshape(1,-1)\n",
    "        agent.memory_recall(observation,action,reward,new_observation,flag)\n",
    "        observation = new_observation\n",
    "        \n",
    "        end = time.time()\n",
    "        t = end-start\n",
    "        if t>20:\n",
    "            flag=True\n",
    "        \n",
    "        total_episode_rewards = sum(agent.episode_rewards)\n",
    "        if total_episode_rewards<-300:\n",
    "            flag = True\n",
    "        \n",
    "        if flag == True:\n",
    "            rewards_over_time.append(total_episode_rewards)\n",
    "            max_reward = np.max(rewards_over_time)\n",
    "            if int(total_episode_rewards)>270 and i>2000:\n",
    "                render=True\n",
    "            episode_max = np.argmax(rewards_over_time)\n",
    "            if total_episode_rewards>=300:\n",
    "                win=win+1\n",
    "            print('##################################################################')\n",
    "            print('Walk# : ',i)\n",
    "            print('Reward : ',int(total_episode_rewards))\n",
    "            print('Time : ',np.round(t,2),'sec')\n",
    "            print('Maximum Reward : '+str(int(max_reward))+'       (in episode#:'+str(episode_max)+')')\n",
    "            print('Wins : '+str(win))\n",
    "\n",
    "            hist,mm = agent.training(16)\n",
    "            #if max_reward > 100: render = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
