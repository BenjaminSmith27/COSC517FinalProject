{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "polished-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "racial-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "#Setting up the environment\n",
    "seed_val = 456\n",
    "env.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interpreted-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_num , action_num, hidden_layer):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.input_layer = nn.Linear(state_num, hidden_layer)\n",
    "        self.h1_layer = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.h2_layer = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_layer, action_num)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        xh = F.relu(self.input_layer(state))\n",
    "        hh1 = F.relu(self.h1_layer(xh))\n",
    "        hh2 = F.tanh(self.h2_layer(hh1))\n",
    "        state_action_values = self.output_layer(hh2)\n",
    "        \n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "painful-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(nn.Module):\n",
    "    def __init__(self, state_dim , action_dim):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.x_layer = nn.Linear(state_dim, 150)\n",
    "        self.h_layer = nn.Linear(150, 120)\n",
    "        self.y_layer = nn.Linear(120, action_dim)\n",
    "        print(self.x_layer)\n",
    "\n",
    "    def forward(self, state):\n",
    "        xh = F.relu(self.x_layer(state))\n",
    "        hh = F.relu(self.h_layer(xh))\n",
    "        state_action_values = self.y_layer(hh)\n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satisfactory-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.qnet = Q_Network(state_dim, action_dim)\n",
    "        self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=0.001)\n",
    "        self.discount_factor = 0.99\n",
    "        self.MSELoss_function = nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        pass\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.action_space.sample()  # choose random action\n",
    "        else:\n",
    "                network_output_to_numpy = self.qnet(state).data.numpy()\n",
    "                return np.argmax(network_output_to_numpy)  # choose greedy action\n",
    "\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        \n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "        qsa_next_action = self.qnet(next_state)\n",
    "        qsa_next_action,_ = torch.max(qsa_next_action, dim=1, keepdim=True)\n",
    "        not_terminals = 1 - terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    \n",
    "    def update_Sarsa_Network(self, state, next_state, action, next_action, reward, terminals):\n",
    "\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "\n",
    "        qsa_next_action = torch.gather(self.qnet(next_state), dim=1, index=next_action.long())\n",
    "\n",
    "        not_terminals = 1 - terminals\n",
    "\n",
    "        qsa_next_target = reward + not_terminals * (self.discount_factor * qsa_next_action)\n",
    "\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "# FIXME\n",
    "    def update_Bpessimistic_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "        \n",
    "        qsa_next_action = self.qnet(next_state)\n",
    "        qsa_best_next_action,_ = torch.max(qsa_next_action, dim=1, keepdim=True)\n",
    "        qsa_worst_next_action,_ = torch.min(qsa_next_action, dim=1, keepdim=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        not_terminals = 1 - terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * ((1-self.beta) * qsa_best_next_action + self.beta * qsa_worst_next_action)\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()     \n",
    "        \n",
    "    def update(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample_minibatch(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            \n",
    "    def update_s(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, next_actions, rewards, terminals = self.replay_buffer.sample_minibatch_sarsa(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            next_actions = torch.Tensor(next_actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Sarsa_Network(states, next_states, actions, next_actions, rewards, terminals)\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        \n",
    "        return np.argmax(self.qnet(state).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "olive-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.buffer_s = []\n",
    "        \n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    def add_to_buffer_sarsa(self, data):\n",
    "        #data must be of the form (state,next_state,action,n_action,reward,terminal)\n",
    "        self.buffer_s.append(data)\n",
    "\n",
    "    def sample_minibatch(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer)-1) \n",
    "            transition = self.buffer[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(rewards), torch.Tensor(terminals)\n",
    "\n",
    "    def sample_minibatch_sarsa(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        next_actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer_s)-1) \n",
    "            transition = self.buffer_s[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            next_actions.append(transition[3])\n",
    "            rewards.append(transition[4])\n",
    "            terminals.append(transition[5])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(next_actions), torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fleet-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_a(values, window=50):\n",
    "    weight = np.repeat(1.0, window)/window\n",
    "    smas = np.convolve(values,weight,'valid')\n",
    "    return smas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "planned-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-toyota",
   "metadata": {},
   "source": [
    "# Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mighty-compromise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 456 sum_of_rewards_for_episode: 162.01339753507645\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2fb799992491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3cecc268cac5>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, update_rate)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mterminals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterminals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_Q_Network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_s\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3cecc268cac5>\u001b[0m in \u001b[0;36mupdate_Q_Network\u001b[1;34m(self, state, next_state, action, reward, terminals)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnet_optim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mq_network_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnet_optim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL525\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL525\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL525\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL525\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_dim, action_dim)\n",
    "number_of_episodes = 600\n",
    "max_time_steps = 2000\n",
    "reward_list = []\n",
    "epsilon = 1\n",
    "finals = []\n",
    "\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    reward_sum = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(max_time_steps):\n",
    "        action = agent.epsilon_greedy_action(torch.from_numpy(state).float() , epsilon)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        agent.replay_buffer.add_to_buffer((state,next_state,[action],[reward],[terminal]))\n",
    "        state = next_state\n",
    "        if terminal:\n",
    "            clear_output(wait=True)\n",
    "            print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum)\n",
    "            reward_list.append(reward_sum)\n",
    "            finals.append(reward)\n",
    "            break\n",
    "            \n",
    "    agent.update(128)\n",
    "    \n",
    "    if epsilon > 0.2:\n",
    "        epsilon *= 0.995 #Epsilon decay\n",
    "    \n",
    "    if epsilon <= 0.2:\n",
    "        epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "smas_ = m_a(reward_list)\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(reward_list)\n",
    "ax.plot(smas_)\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Deep Q learning (600 episodes training)')\n",
    "plt.savefig('DQN600epma', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-literature",
   "metadata": {},
   "source": [
    "# Deep Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_dim, action_dim)\n",
    "number_of_episodes = 600\n",
    "max_time_steps = 2000\n",
    "epsilon = 1\n",
    "reward_list_sarsa = []\n",
    "final_rewards = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for episode in range(number_of_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    reward_sum = 0\n",
    "    \n",
    "    action = agent.epsilon_greedy_action(torch.from_numpy(state).float(), epsilon)\n",
    "    \n",
    "    state_1, reward, terminal, _ = env.step(action)\n",
    "    \n",
    "    #Checks for early Finish\n",
    "    if terminal:\n",
    "        \n",
    "        action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "        agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "        \n",
    "        reward_sum += reward\n",
    "        \n",
    "        final_rewards.append(reward)\n",
    "        \n",
    "        reward_list_sarsa.append(reward_sum)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Early finish!', 'reward =', reward)\n",
    "        print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum, 'final reward', \\\n",
    "                      reward, 'epsilon:', epsilon)\n",
    "        \n",
    "    #If not finished after first action - continue learning\n",
    "    else:\n",
    "        for i in range(max_time_steps):\n",
    "\n",
    "            action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "\n",
    "            state_2, reward_1, terminal_1, _ = env.step(action_1)\n",
    "\n",
    "            agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "\n",
    "            reward_sum += reward\n",
    "\n",
    "            state = state_1\n",
    "            state_1 = state_2\n",
    "            action = action_1\n",
    "            reward = reward_1\n",
    "            terminal = terminal_1\n",
    "\n",
    "            if terminal:\n",
    "\n",
    "                action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "\n",
    "                agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "\n",
    "                reward_sum += reward\n",
    "                \n",
    "                final_rewards.append(reward)\n",
    "                \n",
    "                reward_list_sarsa.append(reward_sum)\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum, 'final reward', \\\n",
    "                      reward, 'epsilon:', epsilon)\n",
    "\n",
    "                \n",
    "                break\n",
    "            \n",
    "    agent.update_s(128) \n",
    "    \n",
    "    if epsilon > 0.2:\n",
    "        epsilon *= 0.995\n",
    "    \n",
    "    if epsilon <= 0.2:\n",
    "        epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "smas = m_a(reward_list_sarsa)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(reward_list_sarsa)\n",
    "ax.plot(smas)\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Deep SARSA learning (600 episodes training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-security",
   "metadata": {},
   "source": [
    "# Deep B-pessimistic Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_dim, action_dim)\n",
    "number_of_episodes = 600\n",
    "max_time_steps = 2000\n",
    "reward_list = []\n",
    "epsilon = 1\n",
    "finals = []\n",
    "\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    reward_sum = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(max_time_steps):\n",
    "        action = agent.epsilon_greedy_action(torch.from_numpy(state).float() , epsilon)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        agent.replay_buffer.add_to_buffer((state,next_state,[action],[reward],[terminal]))\n",
    "        state = next_state\n",
    "        if terminal:\n",
    "            clear_output(wait=True)\n",
    "            print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum)\n",
    "            reward_list.append(reward_sum)\n",
    "            finals.append(reward)\n",
    "            break\n",
    "            \n",
    "    agent.update_b(128)\n",
    "    \n",
    "    if epsilon > 0.2:\n",
    "        epsilon *= 0.995 #Epsilon decay\n",
    "    \n",
    "    if epsilon <= 0.2:\n",
    "        epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(reward_list)\n",
    "ax.plot(smas_)\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Deep B-pessimistic Q learning (600 episodes training)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-letter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-thumb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
