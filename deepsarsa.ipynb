{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governing-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "immediate-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "#Setting up the environment\n",
    "#env = gym.make('CartPole-v1')\n",
    "seed_val = 456\n",
    "env.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indirect-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
    "#env = CliffWalkingEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quality-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_num , action_num, hidden_layer):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.input_layer = nn.Linear(state_num, hidden_layer)\n",
    "        self.h1_layer = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.h2_layer = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_layer, action_num)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        xh = F.relu(self.input_layer(state))\n",
    "        hh1 = F.relu(self.h1_layer(xh))\n",
    "        hh2 = F.tanh(self.h2_layer(hh1))\n",
    "        state_action_values = self.output_layer(hh2)\n",
    "        \n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scenic-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(nn.Module):\n",
    "    def __init__(self, state_dim , action_dim):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.x_layer = nn.Linear(state_dim, 150)\n",
    "        self.h_layer = nn.Linear(150, 120)\n",
    "        self.y_layer = nn.Linear(120, action_dim)\n",
    "        print(self.x_layer)\n",
    "\n",
    "    def forward(self, state):\n",
    "        xh = F.relu(self.x_layer(state))\n",
    "        hh = F.relu(self.h_layer(xh))\n",
    "        state_action_values = self.y_layer(hh)\n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atmospheric-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.qnet = Q_Network(state_dim, action_dim)\n",
    "        self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=0.001)\n",
    "        self.discount_factor = 0.99\n",
    "        self.MSELoss_function = nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        pass\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.action_space.sample()  # choose random action\n",
    "        else:\n",
    "                network_output_to_numpy = self.qnet(state).data.numpy()\n",
    "                return np.argmax(network_output_to_numpy)  # choose greedy action\n",
    "\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        \n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "        qsa_next_action = self.qnet(next_state)\n",
    "        qsa_next_action,_ = torch.max(qsa_next_action, dim=1, keepdim=True)\n",
    "        not_terminals = 1 - terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    \n",
    "    def update_Sarsa_Network(self, state, next_state, action, next_action, reward, terminals):\n",
    "\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "\n",
    "        qsa_next_action = torch.gather(self.qnet(next_state), dim=1, index=next_action.long())\n",
    "\n",
    "        not_terminals = 1 - terminals\n",
    "\n",
    "        qsa_next_target = reward + not_terminals * (self.discount_factor * qsa_next_action)\n",
    "\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    def update(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample_minibatch(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            \n",
    "    def update_s(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, next_actions, rewards, terminals = self.replay_buffer.sample_minibatch_sarsa(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            next_actions = torch.Tensor(next_actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Sarsa_Network(states, next_states, actions, next_actions, rewards, terminals)\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        \n",
    "        return np.argmax(self.qnet(state).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artificial-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.buffer_s = []\n",
    "        \n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    def add_to_buffer_sarsa(self, data):\n",
    "        #data must be of the form (state,next_state,action,n_action,reward,terminal)\n",
    "        self.buffer_s.append(data)\n",
    "\n",
    "    def sample_minibatch(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer)-1) \n",
    "            transition = self.buffer[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(rewards), torch.Tensor(terminals)\n",
    "\n",
    "    def sample_minibatch_sarsa(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        next_actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer_s)-1) \n",
    "            transition = self.buffer_s[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            next_actions.append(transition[3])\n",
    "            rewards.append(transition[4])\n",
    "            terminals.append(transition[5])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(next_actions), torch.Tensor(rewards), torch.Tensor(terminals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "opening-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "consolidated-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 599 sum_of_rewards_for_episode: 139.174990890346 final reward 0.08964737140062518 epsilon: 0.2\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_dim, action_dim)\n",
    "number_of_episodes = 600\n",
    "max_time_steps = 2000\n",
    "epsilon = 1\n",
    "reward_list_sarsa = []\n",
    "final_rewards = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for episode in range(number_of_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    reward_sum = 0\n",
    "    \n",
    "    action = agent.epsilon_greedy_action(torch.from_numpy(state).float(), epsilon)\n",
    "    \n",
    "    state_1, reward, terminal, _ = env.step(action)\n",
    "    \n",
    "    #Checks for early Finish\n",
    "    if terminal:\n",
    "        \n",
    "        action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "        agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "        \n",
    "        reward_sum += reward\n",
    "        \n",
    "        final_rewards.append(reward)\n",
    "        \n",
    "        reward_list_sarsa.append(reward_sum)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Early finish!', 'reward =', reward)\n",
    "        print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum, 'final reward', \\\n",
    "                      reward, 'epsilon:', epsilon)\n",
    "        \n",
    "    #If not finished after first action - continue learning\n",
    "    else:\n",
    "        for i in range(max_time_steps):\n",
    "\n",
    "            action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "\n",
    "            state_2, reward_1, terminal_1, _ = env.step(action_1)\n",
    "\n",
    "            agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "\n",
    "            reward_sum += reward\n",
    "\n",
    "            state = state_1\n",
    "            state_1 = state_2\n",
    "            action = action_1\n",
    "            reward = reward_1\n",
    "            terminal = terminal_1\n",
    "\n",
    "            if terminal:\n",
    "\n",
    "                action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "\n",
    "                agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "\n",
    "                reward_sum += reward\n",
    "                \n",
    "                final_rewards.append(reward)\n",
    "                \n",
    "                reward_list_sarsa.append(reward_sum)\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum, 'final reward', \\\n",
    "                      reward, 'epsilon:', epsilon)\n",
    "\n",
    "                \n",
    "                break\n",
    "            \n",
    "    agent.update_s(128) \n",
    "    \n",
    "    if epsilon > 0.2:\n",
    "        epsilon *= 0.995\n",
    "    \n",
    "    if epsilon <= 0.2:\n",
    "        epsilon = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-guide",
   "metadata": {},
   "source": [
    "def m_a(values, window=50):\n",
    "    weight = np.repeat(1.0, window)/window\n",
    "    smas = np.convolve(values,weight,'valid')\n",
    "    return smas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-perry",
   "metadata": {},
   "source": [
    "smas = m_a(reward_list_sarsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wired-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(reward_list_sarsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coral-settle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-65.86465577492623, -239.8972473775645, -192.16013905484434, -396.9074949026678, -211.6432468663937, -136.8846472951742, -175.39246876763423, -108.93093639462678, -281.0770165150101, -237.64562206172863, -260.0906930422538, -184.71481217265472, -187.80695467925815, -233.45110485829213, -129.7265183771321, -343.0300303554162, -91.1712232911548, -321.2591353803257, -274.5253592775613, -157.71598932002695, -77.19874421226031, -90.20080912111511, -53.68838890041894, -155.91369751780357, -223.38015989990987, -141.7315802835847, -99.7518334457487, -161.06754663304437, -59.903891443770334, -135.02597576696257, -132.7750853689651, -113.63070739376494, -143.2643427769621, -76.23682526804774, -109.98682868947479, -104.16909989540846, -79.99414975283997, -191.66416002918714, -177.71745061933197, -78.9967327475942, -297.75850679271434, -108.36728382366181, -100.84874878154267, -63.97160365579554, -58.2212893055129, -114.40961058531076, -180.54722553538306, -112.10560278804068, -83.9332239598177, -100.9495793033744, -124.39762864846716, -95.92680136914815, -141.99131168150305, -86.36161077908368, -71.2397973811771, -89.45707866087858, -51.01768889172383, -89.42093052589273, -62.60615311368839, -47.092570813880776, -113.63302673095178, -131.96601172703203, -93.09784672195433, -96.4993326771307, -82.00953939034238, -68.88320003689938, -88.02968300269427, -104.24518628701286, -129.29084421117878, -74.05645502780868, -100.80991148080875, -23.420545464908074, -102.11102427589326, -52.28038770753942, -74.05680601992626, -57.0838525802497, -39.67974468597126, -72.02415389673594, -99.27191721392569, -74.43078562720271, -48.63861210134654, -61.65229712713571, -173.82029771138397, -66.53787380195024, -15.992147376001384, -71.34673368913097, -8.390097839520976, -77.59615897420619, -86.22033054190214, -93.60208129457558, -54.25622322074352, -57.44438922849646, -35.350196059135385, -76.55040172662154, -85.9117126975321, -69.11687828982812, -56.63036842833539, -21.05893238945913, -58.966274118750334, -99.0244488168753, -43.206380660018674, -68.02978140026147, -45.55086740746895, -64.78992685184213, -78.75335262102394, -71.76765168269603, -47.47450392072929, -313.09265734619567, -92.07103702657963, -11.397406730472113, -31.176984705309835, -112.46277869225605, -34.58162972545992, -46.583095372271075, -12.06433755050638, -40.448084650737826, -23.465729630223976, -33.45897966326294, -76.79229868347352, -25.041912659265307, -28.382960670108474, -2.4240725418946454, -20.413173795155515, -15.89662408434593, -79.60438073990204, -10.441994960358912, 17.37153330257223, 35.12437788211878, -32.490216678341966, -34.36587241185731, 14.920495323480296, -85.93072287230318, -37.24123762242367, -64.37003420204785, 20.299261971449543, -1.6066922943003874, -61.11229367967684, 1.2011244210920893, -39.87909346047057, -22.447488223099697, -49.38935034162762, -58.292929205369504, -34.61662167220507, -185.35218036746605, -60.74673405639964, -18.458641571173914, -104.33197098839067, -19.32110186259301, 25.4359137124151, -2.5059891358011157, 1.1246057557055593, 31.917346082098646, -1.845334257432384, -24.828464843301575, -33.30965517007584, -38.481577166026184, -95.65659135378621, -58.83215597867748, -38.54379976853106, -74.99604305129824, 86.13786908744578, 33.75199367850644, -105.05731434662184, -100.25183481600794, -45.87147723416639, -40.72042400067892, -23.07169121916573, -5.6453551189727165, -21.63156146702832, -45.45110156753266, -10.99509052464476, -17.976852114397104, -12.773810350277984, -61.571259075883304, -36.03702453669577, 23.209982837258153, -11.519643505925231, 10.244855161407642, 45.33047051399839, -63.44866111297513, -14.91749889095854, 29.811730816739868, -83.12126131644985, -66.48188329590118, -38.39523468075794, -61.706710078707104, 129.36818882400254, -32.29771562407971, -31.55195354865704, -46.19524005906872, -58.68331464797129, -83.85500887515909, -178.7911770852425, -22.40383975940692, 19.689326756966835, -184.37105842260422, -24.37280569017242, 7.70884740686593, -52.766832085484765, 7.643598961907399, 37.25660912614256, 76.34612802800761, 110.72487412260753, 31.785335458164038, 53.26494847641689, -63.15470036997725, -6.523642578753055, 98.564877725655, 54.61388252045277, -41.08583738846727, 38.855486660434124, 10.089238033389003, 55.168659412426024, 143.50307814885218, -6.101352445482219, 80.29617930483043, -26.14471930182593, 54.18400305460682, -37.76252234613744, -82.37293704725697, 6.562516335247245, -17.70311462702639, 42.60758390326268, 125.43042510932192, 162.9060237545018, -15.610003244807704, 67.33421557354772, -63.24495604875326, -57.270785861736925, 35.559848455961415, -15.416149325795631, 86.3591324998477, 16.341110161286004, 102.34451184463154, -51.276336411554006, 98.77287815993111, 37.66535105408788, -59.03282174451116, 17.205241016759, 93.85988022233721, -139.12307567023672, 107.81801766877123, -69.25786447545462, 21.44328218053319, 87.70815390725436, 16.0281201760652, 24.69042434317502, 119.45897359448378, 13.527990290213461, -30.09995308400813, 4.54527286842332, 31.058754844769858, -73.62188425257412, 10.277074704073783, -8.010385288992698, 55.912418179051684, 11.48577491703101, 51.825349318797464, 63.25427416282311, -35.44426793151018, 94.70116251431435, 25.748330177855117, 53.92821968475259, 45.133955008769476, -167.67970637444944, -52.3636575390089, 28.448479601705813, 61.14664432889145, 207.28135800484085, 64.8458315519653, 75.95305433922181, 62.96253137761274, -66.378001175756, 225.59429323853874, 36.38400724016644, 111.6583171200652, 250.569892207227, 7.319586564175155, -199.98382706693025, 89.4463132801915, 15.080351368156574, 98.36781146675145, -216.50153541886388, 35.20666591216593, 172.0764067088208, -40.14852477728688, 34.34373673504544, 98.40316209314257, 203.37628642250098, 32.07722179202234, -101.66324169806458, -107.31074636576679, -22.86499529868138, 27.395616153267042, -8.168294828748477, -58.26256127831198, 87.05548444916082, 60.13198726576492, -50.857366878949286, -130.62644943658245, -43.1506484609742, 110.0131942212363, -24.45107788222738, -10.782728969011771, 46.71225928350966, 53.91704031185488, 98.68159068477635, 85.18042427537026, 41.57778027390865, -24.338656013869084, 218.74045144791063, 16.14495594213007, 92.11666793764999, -397.3808278729536, 71.80280940073915, 48.6848177032241, 141.22496666296513, 73.55995682378975, -80.5784629050817, 103.10478493882047, 59.98226646368772, 78.53314641008541, 2.1873691879078083, -49.02429960173644, -74.47175755686658, 68.53507787857983, 210.33652477412983, -23.683514150077936, 174.69925291360343, 37.98907619525665, 52.263109747927096, 286.8142755133527, 187.3533447758163, 122.5363266381518, 43.68595468820372, 90.97798115833109, 222.55304171407073, 105.80734277804623, -38.01865364305886, 159.1074645253988, 239.36272189147792, -123.23358989984756, 118.75448698389162, 158.36898007627155, 211.29250841643287, 138.26345967398242, 235.781031137373, -23.312623908794166, 203.6180856200553, 104.08230997853592, 201.47575920151573, 240.5347505940727, 0.16455002065612945, 67.39389777711585, 172.6355678960639, -71.97812870339958, 137.18605863336728, 236.5061828387177, 75.05252634115082, 10.17241806092296, 258.6041991956332, 40.37551759937385, 136.35403733945228, 217.49309432034448, 216.07066195807457, 37.42127772227071, 199.94513512285295, 228.41961599203006, 113.5116289103775, 249.83162151429238, 56.85085938921276, 16.410719072758873, 230.0461157784282, 238.87312620795183, -40.79981203480969, 95.78226750119663, 20.009242860688314, 63.839872840184654, 97.7754513931086, -11.373327237129601, -33.91481876291933, 99.58842294924062, 105.00277344104262, -50.124760657853024, 115.06467843319848, 38.4193752213537, 95.59865143269647, 13.915521109447981, 211.53067302934386, 54.86794186014038, 157.09375069146316, -77.79655771412007, 51.15832058215407, 63.5033032928882, 143.25613106679347, 196.7861722476902, -25.0625183154816, 129.5383065922101, 129.89620015907812, 46.85426329452197, 221.58557409767238, 66.24067405320804, 215.93808180421138, 85.20915308370577, 56.84529689924208, 155.68532135516847, 166.6851662049983, 89.60767824332127, 188.60710434234997, 187.24925360769814, 70.32077480204968, 128.06584607586788, 227.09242432026304, 97.71437072371573, 93.55213976890145, 254.11729548423642, 142.91147457835683, 238.62120872755565, 213.4446409843301, 91.98010596698563, 48.63048320274876, -76.42576396896412, 190.92763038133768, 99.42648333320138, 117.16375657320997, 112.9433387220317, 104.72793040986116, 76.8668037094007, 145.25392347871457, 64.67528618857091, 89.96771843375905, 217.2415323758729, 196.8512896416292, -40.53433727714652, 248.4863749592246, 87.96445232034112, 104.37733175416565, 243.84244654465363, 71.11507019636271, 85.96972424091442, 108.77754644220451, 108.1385065370281, 181.54250415358706, 244.77552880841057, 236.88821002785355, -107.79932046562907, 83.21597501523128, -42.52334746658603, 103.07979372425909, 84.9837617338004, 65.76376746236149, 128.5250283217645, 209.50345634300612, 283.87868730544517, 212.9282560594686, 214.4240937364745, 161.04953416322553, 20.033188280057367, 219.15906035846788, 125.73435833377424, 66.54586559926483, 23.341434571342973, 29.577352865577716, 57.02385234320126, 120.0160760822808, 155.44388404250574, 127.74295978881136, 220.71075851137908, 66.47974067361095, 142.35137469406055, 166.42845452226402, 226.51156533381118, 223.79449939842348, 124.71938859662686, -17.158770071471025, 86.3902116415567, 114.10821689544541, 29.23797236259867, 191.58891275142372, 118.80686278237157, 126.954752894787, 231.18557216525417, 129.31352688456326, 60.94983428590096, 260.77547649413304, 137.68309351589264, 203.45174949530093, 257.5045089044006, 189.7862632837995, 117.26210393980278, 248.38131600809308, -0.9439250305632452, 113.27690832974179, -0.18221031132005328, 104.51301647991257, 201.21607828245791, 19.540957784284977, 73.25925206117016, 145.77442837815755, 75.77168165875068, 187.18429571281536, 275.36843164626, 208.0151740198208, 193.45052609398567, 237.73408297186862, 131.71371188677176, 237.22427566225468, 74.07776029068872, 207.6223729200798, 12.399035055582093, 71.91572747912693, 241.54145063926074, 179.91945880031162, 22.375347949649, 59.56000963366176, 100.19115125920088, 263.49063301872286, 243.13598825762085, 77.39158014986778, 118.94206406874568, 190.7093324077885, 210.84339612024004, 127.57540883701867, 131.93669733809526, 132.51418217210974, 56.54858803718896, 118.72035432934759, 129.25325493203368, 217.07519053424983, -2.204044741671212, 248.27719590102876, 192.94152609085083, 290.383127109553, 229.27657175469824, 275.54786034455265, 224.00212749397605, 242.0679221198567, 213.10677977858336, 112.27429454327667, 214.56366662607076, 188.49567229417443, 145.87043587082104, 154.7032492462089, 171.5509403589574, 106.32110050025395, 239.9430886862387, 70.87851921665336, 158.04029081596323, 90.24619467047731, 20.624212714174654, -13.763879824966951, -66.51512962335354, 89.8312535847779, 7.873968146263252, 74.92563159742315, 77.11784294607362, 94.94929877042107, 241.23080815966102, 116.423897029882, 14.11593264170729, 6.036128852970802, 59.02549350116018, 43.13449178341809, 164.4915890591431, 174.65352566757844, 169.8858353533181, 119.80174870533739, 115.80416731420475, 13.274690285771689, 146.55862238201104, 218.17651817854568, 218.9977486907685, 224.71883800966182, 98.2493765607858, 271.938042358674, 211.12724439298972, 72.74112343848647, 116.78634170826494, 128.68634290797456, 109.04438502659006, 109.08280208741992, 106.65484043741421, 216.26847307713945, 49.196473053033024, 127.87920949579436, 213.045218492632, 48.13523708447533, 107.30222338284393, 214.52024528459302, -15.55436369694781, 116.18146045142703, 190.1322160314766, -87.86058414556948, 95.70758278579218, 114.82279071814921, 209.09976289034086, 157.50932661603213, 88.631364595733, 156.76707972367623, 139.174990890346]\n"
     ]
    }
   ],
   "source": [
    "print(reward_list_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_list_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(reward_list_sarsa)\n",
    "#ax.plot(smas)\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Deep SARSA learning (600 episodes training)')\n",
    "#plt.savefig('Sarsa600epma', transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
