{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respective-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cheap-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "#Setting up the environment\n",
    "#env = gym.make('CartPole-v1')\n",
    "seed_val = 456\n",
    "env.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "breathing-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_num , action_num, hidden_layer):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.input_layer = nn.Linear(state_num, hidden_layer)\n",
    "        self.h1_layer = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.h2_layer = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_layer, action_num)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        xh = F.relu(self.input_layer(state))\n",
    "        hh1 = F.relu(self.h1_layer(xh))\n",
    "        hh2 = F.tanh(self.h2_layer(hh1))\n",
    "        state_action_values = self.output_layer(hh2)\n",
    "        \n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ultimate-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(nn.Module):\n",
    "    def __init__(self, state_dim , action_dim):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.x_layer = nn.Linear(state_dim, 150)\n",
    "        self.h_layer = nn.Linear(150, 120)\n",
    "        self.y_layer = nn.Linear(120, action_dim)\n",
    "        print(self.x_layer)\n",
    "\n",
    "    def forward(self, state):\n",
    "        xh = F.relu(self.x_layer(state))\n",
    "        hh = F.relu(self.h_layer(xh))\n",
    "        state_action_values = self.y_layer(hh)\n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "early-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.qnet = Q_Network(state_dim, action_dim)\n",
    "        self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=0.001)\n",
    "        self.discount_factor = 0.99\n",
    "        self.beta = 0.1\n",
    "        self.MSELoss_function = nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        pass\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.action_space.sample()  # choose random action\n",
    "        else:\n",
    "                network_output_to_numpy = self.qnet(state).data.numpy()\n",
    "                return np.argmax(network_output_to_numpy)  # choose greedy action\n",
    "\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        \n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "        qsa_next_action = self.qnet(next_state)\n",
    "        qsa_next_action,_ = torch.max(qsa_next_action, dim=1, keepdim=True)\n",
    "        not_terminals = 1 - terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    \n",
    "    def update_Sarsa_Network(self, state, next_state, action, next_action, reward, terminals):\n",
    "\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "\n",
    "        qsa_next_action = torch.gather(self.qnet(next_state), dim=1, index=next_action.long())\n",
    "\n",
    "        not_terminals = 1 - terminals\n",
    "\n",
    "        qsa_next_target = reward + not_terminals * (self.discount_factor * qsa_next_action)\n",
    "\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "    \n",
    "    # FIXME\n",
    "    def update_Bpessimistic_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "        \n",
    "        qsa_next_action = self.qnet(next_state)\n",
    "        qsa_best_next_action,_ = torch.max(qsa_next_action, dim=1, keepdim=True)\n",
    "        qsa_worst_next_action,_ = torch.min(qsa_next_action, dim=1, keepdim=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        not_terminals = 1 - terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * ((1-self.beta) * qsa_best_next_action + self.beta * qsa_worst_next_action)\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    def update(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample_minibatch(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            \n",
    "    def update_s(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, next_actions, rewards, terminals = self.replay_buffer.sample_minibatch_sarsa(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            next_actions = torch.Tensor(next_actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Sarsa_Network(states, next_states, actions, next_actions, rewards, terminals)\n",
    "    \n",
    "    def update_b(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample_minibatch(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Bpessimistic_Network(states, next_states, actions, rewards, terminals)\n",
    "            \n",
    "    def best_move(self, state):\n",
    "        \n",
    "        return np.argmax(self.qnet(state).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "departmental-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.buffer_s = []\n",
    "        \n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    def add_to_buffer_sarsa(self, data):\n",
    "        #data must be of the form (state,next_state,action,n_action,reward,terminal)\n",
    "        self.buffer_s.append(data)\n",
    "\n",
    "    def sample_minibatch(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer)-1) \n",
    "            transition = self.buffer[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(rewards), torch.Tensor(terminals)\n",
    "\n",
    "    def sample_minibatch_sarsa(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        next_actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer_s)-1) \n",
    "            transition = self.buffer_s[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            next_actions.append(transition[3])\n",
    "            rewards.append(transition[4])\n",
    "            terminals.append(transition[5])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(next_actions), torch.Tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "individual-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_a(values, window=50):\n",
    "    weight = np.repeat(1.0, window)/window\n",
    "    smas = np.convolve(values,weight,'valid')\n",
    "    return smas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "painful-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vital-target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 599 sum_of_rewards_for_episode: -156.35139838099823\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_dim, action_dim)\n",
    "number_of_episodes = 600\n",
    "max_time_steps = 2000\n",
    "reward_list = []\n",
    "epsilon = 1\n",
    "finals = []\n",
    "\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    reward_sum = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(max_time_steps):\n",
    "        action = agent.epsilon_greedy_action(torch.from_numpy(state).float() , epsilon)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        agent.replay_buffer.add_to_buffer((state,next_state,[action],[reward],[terminal]))\n",
    "        state = next_state\n",
    "        if terminal:\n",
    "            clear_output(wait=True)\n",
    "            print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum)\n",
    "            reward_list.append(reward_sum)\n",
    "            finals.append(reward)\n",
    "            break\n",
    "            \n",
    "    agent.update_b(128)\n",
    "    \n",
    "    if epsilon > 0.2:\n",
    "        epsilon *= 0.995 #Epsilon decay\n",
    "    \n",
    "    if epsilon <= 0.2:\n",
    "        epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "promising-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "smas_ = m_a(reward_list)\n",
    "# np.where(smas_> 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sapphire-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cooked-aaron",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-97b7ca196926>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmas_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Rewards'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(reward_list)\n",
    "ax.plot(smas_)\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_title('Deep B-pessimistic Q learning (600 episodes training)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-mississippi",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
