{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "numeric-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "lucky-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
    "env = CliffWalkingEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "powered-factory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "homeless-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_num , action_num, hidden_layer):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.input_layer = nn.Linear(state_num, hidden_layer)\n",
    "        self.h1_layer = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.h2_layer = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_layer, action_num)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        xh = F.relu(self.input_layer(state))\n",
    "        hh1 = F.relu(self.h1_layer(xh))\n",
    "        hh2 = F.tanh(self.h2_layer(hh1))\n",
    "        state_action_values = self.output_layer(hh2)\n",
    "        \n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "surgical-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(nn.Module):\n",
    "    def __init__(self, state_dim , action_dim):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.x_layer = nn.Linear(state_dim, 150)\n",
    "        self.h_layer = nn.Linear(150, 120)\n",
    "        self.y_layer = nn.Linear(120, action_dim)\n",
    "        print(self.x_layer)\n",
    "\n",
    "    def forward(self, state):\n",
    "        xh = F.relu(self.x_layer(state))\n",
    "        hh = F.relu(self.h_layer(xh))\n",
    "        state_action_values = self.y_layer(hh)\n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "devoted-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.qnet = Q_Network(state_dim, action_dim)\n",
    "        self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=0.001)\n",
    "        self.discount_factor = 0.99\n",
    "        self.MSELoss_function = nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        pass\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.action_space.sample()  # choose random action\n",
    "        else:\n",
    "                network_output_to_numpy = self.qnet(state).data.numpy()\n",
    "                return np.argmax(network_output_to_numpy)  # choose greedy action\n",
    "\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        \n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "        qsa_next_action = self.qnet(next_state)\n",
    "        qsa_next_action,_ = torch.max(qsa_next_action, dim=1, keepdim=True)\n",
    "        not_terminals = 1 - terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    \n",
    "    def update_Sarsa_Network(self, state, next_state, action, next_action, reward, terminals):\n",
    "\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "\n",
    "        qsa_next_action = torch.gather(self.qnet(next_state), dim=1, index=next_action.long())\n",
    "\n",
    "        not_terminals = 1 - terminals\n",
    "\n",
    "        qsa_next_target = reward + not_terminals * (self.discount_factor * qsa_next_action)\n",
    "\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    def update(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample_minibatch(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            \n",
    "    def update_s(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, next_actions, rewards, terminals = self.replay_buffer.sample_minibatch_sarsa(64)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            next_actions = torch.Tensor(next_actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Sarsa_Network(states, next_states, actions, next_actions, rewards, terminals)\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        \n",
    "        return np.argmax(self.qnet(state).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "national-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.buffer_s = []\n",
    "        \n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    def add_to_buffer_sarsa(self, data):\n",
    "        #data must be of the form (state,next_state,action,n_action,reward,terminal)\n",
    "        self.buffer_s.append(data)\n",
    "\n",
    "    def sample_minibatch(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer)-1) \n",
    "            transition = self.buffer[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(rewards), torch.Tensor(terminals)\n",
    "\n",
    "    def sample_minibatch_sarsa(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        next_actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer_s)-1) \n",
    "            transition = self.buffer_s[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            next_actions.append(transition[3])\n",
    "            rewards.append(transition[4])\n",
    "            terminals.append(transition[5])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(next_actions), torch.Tensor(rewards), torch.Tensor(terminals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "golden-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(48)\n",
      "48\n",
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "action_dim = env.action_space.n\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.n)\n",
    "print(env.observation_space.shape)\n",
    "print(env.observation_space.n.shape)\n",
    "#state_dim = env.observation_space.shape[0]\n",
    "state_dim = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "artificial-opinion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=48, out_features=150, bias=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got int)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-f7184561551a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mreward_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_greedy_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mstate_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got int)"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_dim, action_dim)\n",
    "number_of_episodes = 600\n",
    "max_time_steps = 2000\n",
    "epsilon = 1\n",
    "reward_list_sarsa = []\n",
    "final_rewards = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for episode in range(number_of_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    reward_sum = 0\n",
    "    \n",
    "    action = agent.epsilon_greedy_action(torch.from_numpy(state).float(), epsilon)\n",
    "    \n",
    "    state_1, reward, terminal, _ = env.step(action)\n",
    "    \n",
    "    #Checks for early Finish\n",
    "    if terminal:\n",
    "        \n",
    "        action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "        agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "        \n",
    "        reward_sum += reward\n",
    "        \n",
    "        final_rewards.append(reward)\n",
    "        \n",
    "        reward_list_sarsa.append(reward_sum)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Early finish!', 'reward =', reward)\n",
    "        print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum, 'final reward', \\\n",
    "                      reward, 'epsilon:', epsilon)\n",
    "        \n",
    "    #If not finished after first action - continue learning\n",
    "    else:\n",
    "        for i in range(max_time_steps):\n",
    "\n",
    "            action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "\n",
    "            state_2, reward_1, terminal_1, _ = env.step(action_1)\n",
    "\n",
    "            agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "\n",
    "            reward_sum += reward\n",
    "\n",
    "            state = state_1\n",
    "            state_1 = state_2\n",
    "            action = action_1\n",
    "            reward = reward_1\n",
    "            terminal = terminal_1\n",
    "\n",
    "            if terminal:\n",
    "\n",
    "                action_1 = agent.epsilon_greedy_action(torch.from_numpy(state_1).float(), epsilon)\n",
    "\n",
    "                agent.replay_buffer.add_to_buffer_sarsa((state, state_1, [action], [action_1], [reward],[terminal]))\n",
    "\n",
    "                reward_sum += reward\n",
    "                \n",
    "                final_rewards.append(reward)\n",
    "                \n",
    "                reward_list_sarsa.append(reward_sum)\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum, 'final reward', \\\n",
    "                      reward, 'epsilon:', epsilon)\n",
    "\n",
    "                \n",
    "                break\n",
    "            \n",
    "    agent.update_s(128) \n",
    "    \n",
    "    if epsilon > 0.2:\n",
    "        epsilon *= 0.995\n",
    "    \n",
    "    if epsilon <= 0.2:\n",
    "        epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "compound-sarah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-inf, inf, (8,), float32)\n",
      "(8,)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape)\n",
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "british-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qs(model, state, step):\n",
    "    return model.predict(state.reshape([1, state.shape[0]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-operation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
